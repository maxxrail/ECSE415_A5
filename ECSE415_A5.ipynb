{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a341e8cf",
   "metadata": {},
   "source": [
    "## Imports and Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9176dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/maxx/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Global Settings\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# For YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# For DeepSORT\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# For plotting/debug (optional)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c092360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0.1 Paths & Constants\n",
    "# ============================================\n",
    "# Change this to the folder that contains \"Object_Tracking\"\n",
    "BASE_DIR = Path(\"/home/maxx/Downloads/ecse-415-video-analysis/Object_Tracking\")   # e.g. Path(\"/home/yourname/data/Object_Tracking\")\n",
    "\n",
    "TASK1_IMAGES_DIR = BASE_DIR / \"Task1\" / \"images\"\n",
    "TASK1_GT_PATH    = BASE_DIR / \"Task1\" / \"gt\" / \"gt.txt\"\n",
    "\n",
    "TASK2_IMAGES_DIR = BASE_DIR / \"Task2\" / \"images\"\n",
    "\n",
    "# Output paths\n",
    "TASK1_INPUT_VIDEO = Path(\"task1_input.mp4\")\n",
    "TASK1_OUTPUT_VIDEO = Path(\"task1.mp4\")\n",
    "TASK2_OUTPUT_VIDEO = Path(\"task2.mp4\")\n",
    "TASK2_COUNTS_CSV = Path(\"task2_count.csv\")\n",
    "\n",
    "FPS_TASK1 = 14   # As required\n",
    "FPS_TASK2 = 14   # You can choose any reasonable FPS (e.g. 14 to match Task1)\n",
    "\n",
    "# YOLO model name (downloaded automatically by ultralytics if not present)\n",
    "YOLO_WEIGHTS = \"yolov8n.pt\"  # or 'yolov8s.pt' if you want stronger model\n",
    "DEVICE = \"cuda\"  # or \"cpu\" if you don't have a GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55f22a",
   "metadata": {},
   "source": [
    "## 1. Data Preparation (Task 1 – images → video @ 14 FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da513638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video: task1_input.mp4 (429 frames at 14 FPS)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Convert Task1 images to video (task1_input.mp4)\n",
    "# ============================================\n",
    "def images_to_video(image_dir: Path, output_path: Path, fps: int = 14):\n",
    "    \"\"\"\n",
    "    Convert all images in image_dir to a video at the given fps.\n",
    "    Assumes images are named so that lexicographic sort is correct frame order\n",
    "    (e.g., 000001.jpg, 000002.jpg, ...).\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    )\n",
    "    assert len(image_files) > 0, f\"No images found in {image_dir}\"\n",
    "\n",
    "    # Read first image to get frame size\n",
    "    first_frame = cv2.imread(str(image_files[0]))\n",
    "    assert first_frame is not None, f\"Could not read first image {image_files[0]}\"\n",
    "\n",
    "    height, width = first_frame.shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    for img_path in image_files:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"Warning: could not read {img_path}, skipping.\")\n",
    "            continue\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Saved video: {output_path} ({len(image_files)} frames at {fps} FPS)\")\n",
    "\n",
    "# Run for Task1\n",
    "images_to_video(TASK1_IMAGES_DIR, TASK1_INPUT_VIDEO, fps=FPS_TASK1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522e94d",
   "metadata": {},
   "source": [
    "## 2. YOLOv8 + DeepSORT Tracking (Task 2 – Task1 video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1387df",
   "metadata": {},
   "source": [
    "### 2.1 Initialize YOLO and DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.1 Initialize YOLOv8 and DeepSORT\n",
    "# ============================================\n",
    "def init_yolo(weights_path: str = YOLO_WEIGHTS, device: str = DEVICE):\n",
    "    \"\"\"\n",
    "    Initialize YOLOv8 model.\n",
    "    \"\"\"\n",
    "    model = YOLO(weights_path)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def init_deepsort():\n",
    "    \"\"\"\n",
    "    Initialize DeepSort tracker from deep_sort_realtime.\n",
    "    Important parameters can be tuned as desired.\n",
    "    \"\"\"\n",
    "    tracker = DeepSort(\n",
    "        max_age=30,\n",
    "        n_init=3,\n",
    "        nn_budget=100,\n",
    "        max_iou_distance=0.7\n",
    "    )\n",
    "    return tracker\n",
    "\n",
    "yolo_model = init_yolo()\n",
    "deepsort_tracker = init_deepsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a58de",
   "metadata": {},
   "source": [
    "### 2.2 Helper: Run tracker on a video & save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ecc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.2 Run YOLOv8 + DeepSORT on a video\n",
    "# ============================================\n",
    "def run_tracking(\n",
    "    input_video_path: Path,\n",
    "    output_video_path: Path,\n",
    "    tracker_txt_out: Path,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 + DeepSORT tracking on a video.\n",
    "\n",
    "    Outputs:\n",
    "      - Annotated video with tracking boxes & IDs\n",
    "      - Text file with tracking results:\n",
    "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(input_video_path))\n",
    "    assert cap.isOpened(), f\"Cannot open {input_video_path}\"\n",
    "\n",
    "    # Get frame size\n",
    "    ret, first_frame = cap.read()\n",
    "    assert ret, \"Could not read first frame\"\n",
    "    height, width = first_frame.shape[:2]\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    # Store tracking results per frame\n",
    "    all_tracks = []  # list of (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height)\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Run YOLO on this frame\n",
    "        # NOTE: results[0] because ultralytics returns list even for single image\n",
    "        results = yolo_model(frame, imgsz=640, conf=0.4, verbose=False)[0]\n",
    "\n",
    "        boxes = results.boxes\n",
    "        if boxes is None or len(boxes) == 0:\n",
    "            detections = []\n",
    "        else:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss  = boxes.cls.cpu().numpy()\n",
    "\n",
    "            detections = []\n",
    "            for bbox, score, cls in zip(xyxy, confs, clss):\n",
    "                # Filter for person class only (COCO: class 0 = 'person')\n",
    "                if int(cls) != 0:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                detections.append(([x1, y1, x2, y2], score, \"person\"))\n",
    "\n",
    "        # Update DeepSORT tracker\n",
    "        tracks = deepsort_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        # Draw and store results\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            l, t, r, b = track.to_ltrb()\n",
    "\n",
    "            # Clamp to image\n",
    "            l = max(0, min(l, width - 1))\n",
    "            r = max(0, min(r, width - 1))\n",
    "            t = max(0, min(t, height - 1))\n",
    "            b = max(0, min(b, height - 1))\n",
    "\n",
    "            bb_left = float(l)\n",
    "            bb_top = float(t)\n",
    "            bb_width = float(r - l)\n",
    "            bb_height = float(b - t)\n",
    "\n",
    "            all_tracks.append(\n",
    "                (frame_idx, int(track_id), bb_left, bb_top, bb_width, bb_height)\n",
    "            )\n",
    "\n",
    "            # Draw\n",
    "            cv2.rectangle(frame, (int(l), int(t)), (int(r), int(b)), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (int(l), int(t) - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Save tracking results to txt\n",
    "    tracker_txt_out = Path(tracker_txt_out)\n",
    "    with tracker_txt_out.open(\"w\") as f:\n",
    "        for (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height) in all_tracks:\n",
    "            f.write(\n",
    "                f\"{frame_idx},{track_id},{bb_left:.2f},{bb_top:.2f},{bb_width:.2f},{bb_height:.2f}\\n\"\n",
    "            )\n",
    "\n",
    "    print(f\"Tracking done. Saved video to {output_video_path}\")\n",
    "    print(f\"Tracking results saved to {tracker_txt_out}\")\n",
    "\n",
    "\n",
    "# Run tracking for Task1\n",
    "TASK1_TRACKS_TXT = Path(\"task1_tracks.txt\")\n",
    "run_tracking(\n",
    "    TASK1_INPUT_VIDEO,\n",
    "    TASK1_OUTPUT_VIDEO,\n",
    "    TASK1_TRACKS_TXT,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps=FPS_TASK1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46ace0",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation: MOTA (Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be77b2",
   "metadata": {},
   "source": [
    "### 3.1 Load ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae07cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1 Load ground truth annotations (Task1/gt/gt.txt)\n",
    "# Using only the first 6 columns:\n",
    "# <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, ...\n",
    "# ============================================\n",
    "def load_gt(gt_path: Path):\n",
    "    \"\"\"\n",
    "    Load ground truth MOT-style annotations into:\n",
    "      gt_by_frame: dict[frame] -> list of dicts:\n",
    "        [{'id': int, 'bbox': np.array([x, y, w, h])}, ...]\n",
    "    \"\"\"\n",
    "    gt_by_frame = defaultdict(list)\n",
    "\n",
    "    with gt_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\",\")\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            frame = int(parts[0])\n",
    "            obj_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            w = float(parts[4])\n",
    "            h = float(parts[5])\n",
    "\n",
    "            gt_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": obj_id,\n",
    "                    \"bbox\": np.array([x, y, w, h], dtype=float),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return gt_by_frame\n",
    "\n",
    "gt_by_frame = load_gt(TASK1_GT_PATH)\n",
    "print(f\"Loaded GT for {len(gt_by_frame)} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5095a93",
   "metadata": {},
   "source": [
    "### 3.2 Load predictions (tracker output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f878415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.2 Load tracking results from our tracker output txt\n",
    "# Format: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "# ============================================\n",
    "def load_predictions(pred_path: Path):\n",
    "    pred_by_frame = defaultdict(list)\n",
    "    with pred_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\",\")\n",
    "            frame = int(parts[0])\n",
    "            track_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            w = float(parts[4])\n",
    "            h = float(parts[5])\n",
    "\n",
    "            pred_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": track_id,\n",
    "                    \"bbox\": np.array([x, y, w, h], dtype=float),\n",
    "                }\n",
    "            )\n",
    "    return pred_by_frame\n",
    "\n",
    "pred_by_frame = load_predictions(TASK1_TRACKS_TXT)\n",
    "print(f\"Loaded predictions for {len(pred_by_frame)} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efd6ff",
   "metadata": {},
   "source": [
    "### 3.3 IoU, Hungarian matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.3 IoU & matching utilities\n",
    "# ============================================\n",
    "def xywh_to_xyxy(box_xywh):\n",
    "    x, y, w, h = box_xywh\n",
    "    return np.array([x, y, x + w, y + h], dtype=float)\n",
    "\n",
    "def compute_iou_matrix(gt_boxes_xywh, pred_boxes_xywh):\n",
    "    \"\"\"\n",
    "    Compute NxM IoU matrix between ground-truth (N) and predicted (M) boxes in xywh format.\n",
    "    \"\"\"\n",
    "    if len(gt_boxes_xywh) == 0 or len(pred_boxes_xywh) == 0:\n",
    "        return np.zeros((len(gt_boxes_xywh), len(pred_boxes_xywh)), dtype=float)\n",
    "\n",
    "    gt_xyxy = np.array([xywh_to_xyxy(b) for b in gt_boxes_xywh])   # [N,4]\n",
    "    pr_xyxy = np.array([xywh_to_xyxy(b) for b in pred_boxes_xywh]) # [M,4]\n",
    "\n",
    "    N = gt_xyxy.shape[0]\n",
    "    M = pr_xyxy.shape[0]\n",
    "    iou = np.zeros((N, M), dtype=float)\n",
    "\n",
    "    for i in range(N):\n",
    "        x1g, y1g, x2g, y2g = gt_xyxy[i]\n",
    "        area_g = (x2g - x1g) * (y2g - y1g)\n",
    "        for j in range(M):\n",
    "            x1p, y1p, x2p, y2p = pr_xyxy[j]\n",
    "            area_p = (x2p - x1p) * (y2p - y1p)\n",
    "\n",
    "            inter_x1 = max(x1g, x1p)\n",
    "            inter_y1 = max(y1g, y1p)\n",
    "            inter_x2 = min(x2g, x2p)\n",
    "            inter_y2 = min(y2g, y2p)\n",
    "\n",
    "            inter_w = max(0.0, inter_x2 - inter_x1)\n",
    "            inter_h = max(0.0, inter_y2 - inter_y1)\n",
    "            inter_area = inter_w * inter_h\n",
    "\n",
    "            union_area = area_g + area_p - inter_area\n",
    "            if union_area <= 0:\n",
    "                iou[i, j] = 0.0\n",
    "            else:\n",
    "                iou[i, j] = inter_area / union_area\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecceae6",
   "metadata": {},
   "source": [
    "### 3.4 Compute MOTA, FP, FN, IDSW, GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.4 Compute MOTA, FP, FN, IDSW, GT\n",
    "# ============================================\n",
    "def compute_mota(gt_by_frame, pred_by_frame, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute MOTA, and totals of FP, FN, IDSW, and GT.\n",
    "    Following the definition given in the assignment.\n",
    "    \"\"\"\n",
    "    frames = sorted(gt_by_frame.keys())  # frames with GT; we can also union with pred_by_frame\n",
    "    all_frames = sorted(set(frames).union(pred_by_frame.keys()))\n",
    "\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "    total_IDSW = 0\n",
    "    total_GT = 0\n",
    "\n",
    "    # For ID switch tracking: gt_id -> last matched pred_id\n",
    "    prev_match_for_gt = {}\n",
    "\n",
    "    for t in all_frames:\n",
    "        gt_objs = gt_by_frame.get(t, [])\n",
    "        pr_objs = pred_by_frame.get(t, [])\n",
    "\n",
    "        gt_boxes = [g[\"bbox\"] for g in gt_objs]\n",
    "        gt_ids = [g[\"id\"] for g in gt_objs]\n",
    "\n",
    "        pr_boxes = [p[\"bbox\"] for p in pr_objs]\n",
    "        pr_ids = [p[\"id\"] for p in pr_objs]\n",
    "\n",
    "        N = len(gt_boxes)\n",
    "        M = len(pr_boxes)\n",
    "\n",
    "        total_GT += N\n",
    "\n",
    "        if N == 0 and M == 0:\n",
    "            # nothing here\n",
    "            continue\n",
    "\n",
    "        # IoU matrix\n",
    "        iou_mat = compute_iou_matrix(gt_boxes, pr_boxes)\n",
    "\n",
    "        if N > 0 and M > 0:\n",
    "            # Cost matrix for Hungarian: we want to maximize IoU,\n",
    "            # so we minimize (1 - IoU). Set cost very high if IoU < threshold.\n",
    "            cost = 1.0 - iou_mat\n",
    "            cost[iou_mat < iou_threshold] = 1e6\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_gt_idx = set()\n",
    "            matched_pr_idx = set()\n",
    "\n",
    "            # Evaluate matches above threshold\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if iou_mat[r, c] >= iou_threshold:\n",
    "                    matched_gt_idx.add(r)\n",
    "                    matched_pr_idx.add(c)\n",
    "\n",
    "                    gt_id = gt_ids[r]\n",
    "                    pr_id = pr_ids[c]\n",
    "\n",
    "                    # Identity switch?\n",
    "                    if gt_id in prev_match_for_gt:\n",
    "                        if prev_match_for_gt[gt_id] != pr_id:\n",
    "                            total_IDSW += 1\n",
    "                    prev_match_for_gt[gt_id] = pr_id\n",
    "\n",
    "            # FN: GT with no match\n",
    "            FN_t = N - len(matched_gt_idx)\n",
    "\n",
    "            # FP: predictions with no match\n",
    "            FP_t = M - len(matched_pr_idx)\n",
    "\n",
    "        elif N == 0 and M > 0:\n",
    "            # All predictions are FP\n",
    "            FP_t = M\n",
    "            FN_t = 0\n",
    "\n",
    "        elif N > 0 and M == 0:\n",
    "            # All GT are FN\n",
    "            FN_t = N\n",
    "            FP_t = 0\n",
    "\n",
    "        total_FN += FN_t\n",
    "        total_FP += FP_t\n",
    "\n",
    "    if total_GT == 0:\n",
    "        mota = 0.0\n",
    "    else:\n",
    "        mota = 1.0 - (total_FN + total_FP + total_IDSW) / total_GT\n",
    "\n",
    "    return mota, total_FP, total_FN, total_IDSW, total_GT\n",
    "\n",
    "\n",
    "mota, total_FP, total_FN, total_IDSW, total_GT = compute_mota(\n",
    "    gt_by_frame, pred_by_frame, iou_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"MOTA: {mota:.4f}\")\n",
    "print(f\"Total GT:   {total_GT}\")\n",
    "print(f\"Total FP:   {total_FP}\")\n",
    "print(f\"Total FN:   {total_FN}\")\n",
    "print(f\"Total IDSW: {total_IDSW}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01be02",
   "metadata": {},
   "source": [
    "## 4. Prediction & Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62542e3",
   "metadata": {},
   "source": [
    "### 4.1 Convert Task2 images to a video and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.1 Run YOLOv8 + DeepSORT on Task2 images\n",
    "#     Save annotated video (task2.mp4) and counts per frame\n",
    "# ============================================\n",
    "def track_on_image_sequence(\n",
    "    image_dir: Path,\n",
    "    output_video_path: Path,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps: int = 14,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 + DeepSORT on a sequence of images in image_dir.\n",
    "    Saves an annotated video and returns per-frame counts (dict: frame_idx -> count).\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    )\n",
    "    assert len(image_files) > 0, f\"No images found in {image_dir}\"\n",
    "\n",
    "    # Read first image to get size\n",
    "    first_frame = cv2.imread(str(image_files[0]))\n",
    "    assert first_frame is not None, f\"Could not read first image {image_files[0]}\"\n",
    "    height, width = first_frame.shape[:2]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    frame_counts = {}  # frame_idx (1-based) -> count of people\n",
    "\n",
    "    for idx, img_path in enumerate(image_files, start=1):\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"Warning: could not read {img_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # YOLO inference\n",
    "        results = yolo_model(frame, imgsz=640, conf=0.4, verbose=False)[0]\n",
    "        boxes = results.boxes\n",
    "\n",
    "        detections = []\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss  = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for bbox, score, cls in zip(xyxy, confs, clss):\n",
    "                if int(cls) != 0:   # person class only\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                detections.append(([x1, y1, x2, y2], score, \"person\"))\n",
    "\n",
    "        tracks = deepsort_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        # Count people as number of confirmed tracks in this frame\n",
    "        count = 0\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            count += 1\n",
    "            track_id = track.track_id\n",
    "            l, t, r, b = track.to_ltrb()\n",
    "\n",
    "            # Draw\n",
    "            cv2.rectangle(frame, (int(l), int(t)), (int(r), int(b)), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (int(l), int(t) - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        frame_counts[idx] = count\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Saved Task2 annotated video to {output_video_path}\")\n",
    "    return frame_counts\n",
    "\n",
    "\n",
    "# (Re)initialize YOLO + DeepSORT for Task2 if you want a fresh tracker\n",
    "yolo_model_task2 = init_yolo()\n",
    "deepsort_task2 = init_deepsort()\n",
    "\n",
    "task2_frame_counts = track_on_image_sequence(\n",
    "    TASK2_IMAGES_DIR,\n",
    "    TASK2_OUTPUT_VIDEO,\n",
    "    yolo_model_task2,\n",
    "    deepsort_task2,\n",
    "    fps=FPS_TASK2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ede7cc",
   "metadata": {},
   "source": [
    "### 4.2 Save frame counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.2 Save frame counts to CSV for Kaggle\n",
    "#     Format:\n",
    "#       Number,Count\n",
    "#       1,12\n",
    "#       2,15\n",
    "#       ...\n",
    "# ============================================\n",
    "def save_counts_to_csv(frame_counts: dict, csv_path: Path):\n",
    "    \"\"\"\n",
    "    frame_counts: dict[frame_idx] -> count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for frame_idx in sorted(frame_counts.keys()):\n",
    "        data.append({\"Number\": frame_idx, \"Count\": frame_counts[frame_idx]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved counts to {csv_path}\")\n",
    "\n",
    "save_counts_to_csv(task2_frame_counts, TASK2_COUNTS_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
