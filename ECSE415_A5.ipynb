{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a341e8cf",
   "metadata": {},
   "source": [
    "## Imports and Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9176dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Global Settings\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# DeepSORT\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Plotting / debug (optional)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c092360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0.1 Paths & Constants\n",
    "# ============================================\n",
    "BASE_DIR = Path(\"Object_Tracking\")\n",
    "\n",
    "TASK1_IMAGES_DIR = BASE_DIR / \"Task1\" / \"images\"\n",
    "TASK1_GT_PATH    = BASE_DIR / \"Task1\" / \"gt\" / \"gt.txt\"\n",
    "\n",
    "TASK2_IMAGES_DIR = BASE_DIR / \"Task2\" / \"images\"\n",
    "\n",
    "# Output paths\n",
    "TASK1_INPUT_VIDEO  = Path(\"task1_input.mp4\")\n",
    "TASK1_OUTPUT_VIDEO = Path(\"task1.mp4\")\n",
    "TASK2_OUTPUT_VIDEO = Path(\"task2.mp4\")\n",
    "TASK2_COUNTS_CSV   = Path(\"task2_count.csv\")\n",
    "\n",
    "FPS_TASK1 = 14\n",
    "FPS_TASK2 = 14\n",
    "\n",
    "# YOLO weights\n",
    "YOLO_WEIGHTS = \"yolov8n.pt\"  # or 'yolov8s.pt' if you want a heavier model\n",
    "\n",
    "# Pick CUDA if available, otherwise CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55f22a",
   "metadata": {},
   "source": [
    "## 1. Data Preparation (Task 1 – images → video @ 14 FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da513638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video: task1_input.mp4 (429 frames at 14 FPS)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Convert Task1 images to video (task1_input.mp4)\n",
    "# ============================================\n",
    "def images_to_video(image_dir: Path, output_path: Path, fps: int = 14):\n",
    "    \"\"\"\n",
    "    Convert all images in image_dir to a video at the given fps.\n",
    "    Assumes images are named so that lexicographic sort is correct frame order\n",
    "    (e.g., 000001.jpg, 000002.jpg, ...).\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    )\n",
    "    assert len(image_files) > 0, f\"No images found in {image_dir}\"\n",
    "\n",
    "    # Read first image to get frame size\n",
    "    first_frame = cv2.imread(str(image_files[0]))\n",
    "    assert first_frame is not None, f\"Could not read first image {image_files[0]}\"\n",
    "\n",
    "    height, width = first_frame.shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    for img_path in image_files:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"Warning: could not read {img_path}, skipping.\")\n",
    "            continue\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Saved video: {output_path} ({len(image_files)} frames at {fps} FPS)\")\n",
    "\n",
    "# Run for Task1\n",
    "images_to_video(TASK1_IMAGES_DIR, TASK1_INPUT_VIDEO, fps=FPS_TASK1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522e94d",
   "metadata": {},
   "source": [
    "## 2. YOLOv8 + DeepSORT Tracking (Task 2 – Task1 video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1387df",
   "metadata": {},
   "source": [
    "### 2.1 Initialize YOLO and DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "811a8bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 33.5MB/s 0.2s/s 0.1s<0.5s\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2.1 Initialize YOLOv8 and DeepSORT\n",
    "# ============================================\n",
    "def init_yolo(weights_path: str = YOLO_WEIGHTS, device: str = DEVICE):\n",
    "    \"\"\"\n",
    "    Initialize YOLOv8 model on CPU or CUDA if available.\n",
    "    \"\"\"\n",
    "    model = YOLO(weights_path)\n",
    "    if device != \"cpu\":\n",
    "        model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_deepsort():\n",
    "    \"\"\"\n",
    "    Initialize DeepSort tracker from deep_sort_realtime.\n",
    "    \"\"\"\n",
    "    tracker = DeepSort(\n",
    "        max_age=30,\n",
    "        n_init=3,\n",
    "        nn_budget=100,\n",
    "        max_iou_distance=0.7,\n",
    "    )\n",
    "    return tracker\n",
    "\n",
    "\n",
    "yolo_model = init_yolo()\n",
    "deepsort_tracker = init_deepsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a58de",
   "metadata": {},
   "source": [
    "### 2.2 Helper: Run tracker on a video & save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ecc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking done. Saved video to task1.mp4\n",
      "Tracking results saved to task1_tracks.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2.2 Run YOLOv8 + DeepSORT on a video (Task1)\n",
    "# ============================================\n",
    "def run_tracking(\n",
    "    input_video_path: Path,\n",
    "    output_video_path: Path,\n",
    "    tracker_txt_out: Path,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 + DeepSORT tracking on a video.\n",
    "\n",
    "    Outputs:\n",
    "      - Annotated video with tracking boxes & IDs\n",
    "      - Text file with tracking results:\n",
    "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(input_video_path))\n",
    "    assert cap.isOpened(), f\"Cannot open {input_video_path}\"\n",
    "\n",
    "    # Get frame size from first frame\n",
    "    ret, first_frame = cap.read()\n",
    "    assert ret, \"Could not read first frame\"\n",
    "    height, width = first_frame.shape[:2]\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    all_tracks = []  # (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height)\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_idx += 1\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # YOLO inference (more generous settings)\n",
    "        results = yolo_model(frame, imgsz=960, conf=0.2, verbose=False)[0]\n",
    "        boxes = results.boxes\n",
    "\n",
    "        detections = []\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for bbox, score, cls in zip(xyxy, confs, clss):\n",
    "                # COCO class 0 = 'person'\n",
    "                if int(cls) != 0:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = bbox\n",
    "\n",
    "                # DeepSORT expects [x, y, w, h] (top-left + width/height)\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                detections.append(([x1, y1, w, h], float(score), \"person\"))\n",
    "\n",
    "        tracks = deepsort_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 0:\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "\n",
    "            # Use original detection box for better IoU with GT\n",
    "            l, t, r, b = track.to_ltrb(orig=True)\n",
    "\n",
    "            # Clamp to image\n",
    "            l = max(0, min(int(l), width - 1))\n",
    "            r = max(0, min(int(r), width - 1))\n",
    "            t = max(0, min(int(t), height - 1))\n",
    "            b = max(0, min(int(b), height - 1))\n",
    "\n",
    "            bb_left = float(l)\n",
    "            bb_top = float(t)\n",
    "            bb_width = float(r - l)\n",
    "            bb_height = float(b - t)\n",
    "\n",
    "            all_tracks.append(\n",
    "                (frame_idx, int(track_id), bb_left, bb_top, bb_width, bb_height)\n",
    "            )\n",
    "\n",
    "            # Draw\n",
    "            cv2.rectangle(frame, (l, t), (r, b), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (l, t - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Save tracking results to txt\n",
    "    tracker_txt_out = Path(tracker_txt_out)\n",
    "    with tracker_txt_out.open(\"w\") as f:\n",
    "        for (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height) in all_tracks:\n",
    "            f.write(\n",
    "                f\"{frame_idx},{track_id},{bb_left:.2f},{bb_top:.2f},{bb_width:.2f},{bb_height:.2f}\\n\"\n",
    "            )\n",
    "\n",
    "    print(f\"Tracking done. Saved video to {output_video_path}\")\n",
    "    print(f\"Tracking results saved to {tracker_txt_out}\")\n",
    "\n",
    "\n",
    "# Run tracking for Task1\n",
    "TASK1_TRACKS_TXT = Path(\"task1_tracks.txt\")\n",
    "run_tracking(\n",
    "    TASK1_INPUT_VIDEO,\n",
    "    TASK1_OUTPUT_VIDEO,\n",
    "    TASK1_TRACKS_TXT,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps=FPS_TASK1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46ace0",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation: MOTA (Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be77b2",
   "metadata": {},
   "source": [
    "### 3.1 Load ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ae07cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GT frames: 429\n",
      "Total GT boxes: 19870\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3.1 Load ground truth annotations (Task1/gt/gt.txt)\n",
    "# Using only the first 6 columns:\n",
    "# <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, ...\n",
    "# ============================================\n",
    "def load_gt(gt_path: Path):\n",
    "    \"\"\"\n",
    "    Load ground truth from MOT-style gt.txt.\n",
    "\n",
    "    Assumes columns:\n",
    "      1: frame\n",
    "      2: id\n",
    "      3: bb_left\n",
    "      4: bb_top\n",
    "      5: bb_width\n",
    "      6: bb_height\n",
    "      [7: conf (optional)]\n",
    "      [8: class (optional, 1 = pedestrian)]\n",
    "      [9+: other fields, ignored]\n",
    "\n",
    "    We:\n",
    "      - skip lines with conf <= 0 (unlabeled / ignored)\n",
    "      - if class column exists, keep only class == 1 (pedestrians)\n",
    "    \"\"\"\n",
    "    gt_by_frame = defaultdict(list)\n",
    "\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\",\")\n",
    "            if len(cols) < 6:\n",
    "                continue\n",
    "\n",
    "            frame = int(cols[0])\n",
    "            obj_id = int(cols[1])\n",
    "            bb_left   = float(cols[2])\n",
    "            bb_top    = float(cols[3])\n",
    "            bb_width  = float(cols[4])\n",
    "            bb_height = float(cols[5])\n",
    "\n",
    "            # Optional 7th column: conf\n",
    "            if len(cols) >= 7:\n",
    "                conf = float(cols[6])\n",
    "                # MOT convention: conf <= 0 => ignore\n",
    "                if conf <= 0:\n",
    "                    continue\n",
    "\n",
    "            # Optional 8th column: class (1 = pedestrian)\n",
    "            if len(cols) >= 8:\n",
    "                cls = int(cols[7])\n",
    "                if cls != 1:\n",
    "                    # keep only pedestrians\n",
    "                    continue\n",
    "\n",
    "            gt_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": obj_id,\n",
    "                    \"bbox\": [bb_left, bb_top, bb_width, bb_height],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return gt_by_frame\n",
    "\n",
    "gt_by_frame = load_gt(TASK1_GT_PATH)\n",
    "print(\"Loaded GT frames:\", len(gt_by_frame))\n",
    "print(\"Total GT boxes:\", sum(len(v) for v in gt_by_frame.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5095a93",
   "metadata": {},
   "source": [
    "### 3.2 Load predictions (tracker output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f878415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded predictions for 427 frames\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3.2 Load tracking results from our tracker output txt\n",
    "# Format: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "# ============================================\n",
    "def load_predictions(pred_path: Path):\n",
    "    pred_by_frame = defaultdict(list)\n",
    "    with pred_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\",\")\n",
    "            frame = int(parts[0])\n",
    "            track_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            w = float(parts[4])\n",
    "            h = float(parts[5])\n",
    "\n",
    "            pred_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": track_id,\n",
    "                    \"bbox\": np.array([x, y, w, h], dtype=float),\n",
    "                }\n",
    "            )\n",
    "    return pred_by_frame\n",
    "\n",
    "pred_by_frame = load_predictions(TASK1_TRACKS_TXT)\n",
    "print(f\"Loaded predictions for {len(pred_by_frame)} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efd6ff",
   "metadata": {},
   "source": [
    "### 3.3 IoU, Hungarian matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6fd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.3 IoU & matching utilities\n",
    "# ============================================\n",
    "def xywh_to_xyxy(box_xywh):\n",
    "    \"\"\"Convert [x, y, w, h] -> [x1, y1, x2, y2].\"\"\"\n",
    "    x, y, w, h = box_xywh\n",
    "    return np.array([x, y, x + w, y + h], dtype=float)\n",
    "\n",
    "\n",
    "def compute_iou_matrix(gt_boxes_xywh, pred_boxes_xywh):\n",
    "    \"\"\"\n",
    "    Compute IoU matrix between:\n",
    "      - gt_boxes_xywh: list of [x, y, w, h]\n",
    "      - pred_boxes_xywh: list of [x, y, w, h]\n",
    "    Returns: (N_gt, N_pred) IoU matrix.\n",
    "    \"\"\"\n",
    "    N = len(gt_boxes_xywh)\n",
    "    M = len(pred_boxes_xywh)\n",
    "\n",
    "    if N == 0 or M == 0:\n",
    "        return np.zeros((N, M), dtype=float)\n",
    "\n",
    "    gt = np.array([xywh_to_xyxy(b) for b in gt_boxes_xywh], dtype=float)  # (N,4)\n",
    "    pr = np.array([xywh_to_xyxy(b) for b in pred_boxes_xywh], dtype=float)  # (M,4)\n",
    "\n",
    "    gt_x1 = gt[:, 0][:, None]\n",
    "    gt_y1 = gt[:, 1][:, None]\n",
    "    gt_x2 = gt[:, 2][:, None]\n",
    "    gt_y2 = gt[:, 3][:, None]\n",
    "\n",
    "    pr_x1 = pr[:, 0][None, :]\n",
    "    pr_y1 = pr[:, 1][None, :]\n",
    "    pr_x2 = pr[:, 2][None, :]\n",
    "    pr_y2 = pr[:, 3][None, :]\n",
    "\n",
    "    inter_x1 = np.maximum(gt_x1, pr_x1)\n",
    "    inter_y1 = np.maximum(gt_y1, pr_y1)\n",
    "    inter_x2 = np.minimum(gt_x2, pr_x2)\n",
    "    inter_y2 = np.minimum(gt_y2, pr_y2)\n",
    "\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, a_min=0, a_max=None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, a_min=0, a_max=None)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)   # (N,1)\n",
    "    pr_area = (pr_x2 - pr_x1) * (pr_y2 - pr_y1)   # (1,M)\n",
    "    union_area = gt_area + pr_area - inter_area\n",
    "\n",
    "    iou = np.zeros_like(inter_area)\n",
    "    mask = union_area > 0\n",
    "    iou[mask] = inter_area[mask] / union_area[mask]\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecceae6",
   "metadata": {},
   "source": [
    "### 3.4 Compute MOTA, FP, FN, IDSW, GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63f4ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTA: 0.3938\n",
      "Total GT:   19870\n",
      "Total FP:   1237\n",
      "Total FN:   10436\n",
      "Total IDSW: 372\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3.4 Compute MOTA, FP, FN, IDSW, GT\n",
    "# ============================================\n",
    "def compute_mota(gt_by_frame, pred_by_frame, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute MOTA, and totals of FP, FN, IDSW, and GT.\n",
    "    Following the definition given in the assignment.\n",
    "    \"\"\"\n",
    "    frames = sorted(gt_by_frame.keys())       # frames with GT\n",
    "    all_frames = frames                       # just use GT frames\n",
    "\n",
    "\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "    total_IDSW = 0\n",
    "    total_GT = 0\n",
    "\n",
    "    # For ID switch tracking: gt_id -> last matched pred_id\n",
    "    prev_match_for_gt = {}\n",
    "\n",
    "    for t in all_frames:\n",
    "        gt_objs = gt_by_frame.get(t, [])\n",
    "        pr_objs = pred_by_frame.get(t, [])\n",
    "\n",
    "        gt_boxes = [g[\"bbox\"] for g in gt_objs]\n",
    "        gt_ids = [g[\"id\"] for g in gt_objs]\n",
    "\n",
    "        pr_boxes = [p[\"bbox\"] for p in pr_objs]\n",
    "        pr_ids = [p[\"id\"] for p in pr_objs]\n",
    "\n",
    "        N = len(gt_boxes)\n",
    "        M = len(pr_boxes)\n",
    "\n",
    "        total_GT += N\n",
    "\n",
    "        if N == 0 and M == 0:\n",
    "            # nothing here\n",
    "            continue\n",
    "\n",
    "        # IoU matrix\n",
    "        iou_mat = compute_iou_matrix(gt_boxes, pr_boxes)\n",
    "\n",
    "        if N > 0 and M > 0:\n",
    "            # Cost matrix for Hungarian: we want to maximize IoU,\n",
    "            # so we minimize (1 - IoU). Set cost very high if IoU < threshold.\n",
    "            cost = 1.0 - iou_mat\n",
    "            cost[iou_mat < iou_threshold] = 1e6\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_gt_idx = set()\n",
    "            matched_pr_idx = set()\n",
    "\n",
    "            # Evaluate matches above threshold\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if iou_mat[r, c] >= iou_threshold:\n",
    "                    matched_gt_idx.add(r)\n",
    "                    matched_pr_idx.add(c)\n",
    "\n",
    "                    gt_id = gt_ids[r]\n",
    "                    pr_id = pr_ids[c]\n",
    "\n",
    "                    # Identity switch?\n",
    "                    if gt_id in prev_match_for_gt:\n",
    "                        if prev_match_for_gt[gt_id] != pr_id:\n",
    "                            total_IDSW += 1\n",
    "                    prev_match_for_gt[gt_id] = pr_id\n",
    "\n",
    "            # FN: GT with no match\n",
    "            FN_t = N - len(matched_gt_idx)\n",
    "\n",
    "            # FP: predictions with no match\n",
    "            FP_t = M - len(matched_pr_idx)\n",
    "\n",
    "        elif N == 0 and M > 0:\n",
    "            # All predictions are FP\n",
    "            FP_t = M\n",
    "            FN_t = 0\n",
    "\n",
    "        elif N > 0 and M == 0:\n",
    "            # All GT are FN\n",
    "            FN_t = N\n",
    "            FP_t = 0\n",
    "\n",
    "        total_FN += FN_t\n",
    "        total_FP += FP_t\n",
    "\n",
    "    if total_GT == 0:\n",
    "        mota = 0.0\n",
    "    else:\n",
    "        mota = 1.0 - (total_FN + total_FP + total_IDSW) / total_GT\n",
    "\n",
    "    return mota, total_FP, total_FN, total_IDSW, total_GT\n",
    "\n",
    "\n",
    "mota, total_FP, total_FN, total_IDSW, total_GT = compute_mota(\n",
    "    gt_by_frame, pred_by_frame, iou_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"MOTA: {mota:.4f}\")\n",
    "print(f\"Total GT:   {total_GT}\")\n",
    "print(f\"Total FP:   {total_FP}\")\n",
    "print(f\"Total FN:   {total_FN}\")\n",
    "print(f\"Total IDSW: {total_IDSW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c84cb4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with GT: 429\n",
      "Total GT boxes: 19870 -> avg per frame: 46.31701631701632\n",
      "Total pred boxes on those frames: 10671 -> avg per frame: 24.874125874125873\n"
     ]
    }
   ],
   "source": [
    "# Quick debug: how many GT vs predictions per frame?\n",
    "all_gt_frames = sorted(gt_by_frame.keys())\n",
    "all_pred_frames = sorted(pred_by_frame.keys())\n",
    "\n",
    "total_GT = sum(len(gt_by_frame[f]) for f in all_gt_frames)\n",
    "total_pred = sum(len(pred_by_frame.get(f, [])) for f in all_gt_frames)\n",
    "\n",
    "print(\"Frames with GT:\", len(all_gt_frames))\n",
    "print(\"Total GT boxes:\", total_GT, \"-> avg per frame:\", total_GT / len(all_gt_frames))\n",
    "print(\"Total pred boxes on those frames:\", total_pred, \"-> avg per frame:\", total_pred / len(all_gt_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01be02",
   "metadata": {},
   "source": [
    "## 4. Prediction & Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62542e3",
   "metadata": {},
   "source": [
    "### 4.1 Convert Task2 images to a video and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca5c1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Task2 annotated video to task2.mp4\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4.1 Run YOLOv8 + DeepSORT on Task2 images\n",
    "#     Save annotated video (task2.mp4) and counts per frame\n",
    "# ============================================\n",
    "def track_on_image_sequence(\n",
    "    image_dir: Path,\n",
    "    output_video_path: Path,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps: int = 14,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 + DeepSORT on a sequence of images in image_dir.\n",
    "    Saves an annotated video and returns per-frame counts (dict: frame_idx -> count).\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    )\n",
    "    assert len(image_files) > 0, f\"No images found in {image_dir}\"\n",
    "\n",
    "    # Read first image to get size\n",
    "    first_frame = cv2.imread(str(image_files[0]))\n",
    "    assert first_frame is not None, f\"Could not read first image {image_files[0]}\"\n",
    "    height, width = first_frame.shape[:2]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    frame_counts = {}  # frame_idx (1-based) -> count of people\n",
    "\n",
    "    for idx, img_path in enumerate(image_files, start=1):\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"Warning: could not read {img_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # YOLO inference\n",
    "        results = yolo_model(frame, imgsz=640, conf=0.4, verbose=False)[0]\n",
    "        boxes = results.boxes\n",
    "\n",
    "        detections = []\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss  = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for bbox, score, cls in zip(xyxy, confs, clss):\n",
    "                if int(cls) != 0:   # person class only\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                detections.append(([x1, y1, x2, y2], score, \"person\"))\n",
    "\n",
    "        tracks = deepsort_tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        # Count people as number of confirmed tracks in this frame\n",
    "        count = 0\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            count += 1\n",
    "            track_id = track.track_id\n",
    "            l, t, r, b = track.to_ltrb()\n",
    "\n",
    "            # Draw\n",
    "            cv2.rectangle(frame, (int(l), int(t)), (int(r), int(b)), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"ID {track_id}\",\n",
    "                (int(l), int(t) - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        frame_counts[idx] = count\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Saved Task2 annotated video to {output_video_path}\")\n",
    "    return frame_counts\n",
    "\n",
    "\n",
    "# (Re)initialize YOLO + DeepSORT for Task2 if you want a fresh tracker\n",
    "yolo_model_task2 = init_yolo()\n",
    "deepsort_task2 = init_deepsort()\n",
    "\n",
    "task2_frame_counts = track_on_image_sequence(\n",
    "    TASK2_IMAGES_DIR,\n",
    "    TASK2_OUTPUT_VIDEO,\n",
    "    yolo_model_task2,\n",
    "    deepsort_task2,\n",
    "    fps=FPS_TASK2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ede7cc",
   "metadata": {},
   "source": [
    "### 4.2 Save frame counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70812d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved counts to task2_count.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4.2 Save frame counts to CSV for Kaggle\n",
    "#     Format:\n",
    "#       Number,Count\n",
    "#       1,12\n",
    "#       2,15\n",
    "#       ...\n",
    "# ============================================\n",
    "def save_counts_to_csv(frame_counts: dict, csv_path: Path):\n",
    "    \"\"\"\n",
    "    frame_counts: dict[frame_idx] -> count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for frame_idx in sorted(frame_counts.keys()):\n",
    "        data.append({\"Number\": frame_idx, \"Count\": frame_counts[frame_idx]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved counts to {csv_path}\")\n",
    "\n",
    "save_counts_to_csv(task2_frame_counts, TASK2_COUNTS_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
