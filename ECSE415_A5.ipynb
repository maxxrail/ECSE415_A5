{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a341e8cf",
   "metadata": {},
   "source": [
    "## Imports and Setup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Global Settings\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# DeepSORT\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Plotting / debug (optional)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c092360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 0.1 Paths & Constants\n",
    "# ============================================\n",
    "BASE_DIR = Path(\"Object_Tracking\")\n",
    "\n",
    "TASK1_IMAGES_DIR = BASE_DIR / \"Task1\" / \"images\"\n",
    "TASK1_GT_PATH    = BASE_DIR / \"Task1\" / \"gt\" / \"gt.txt\"\n",
    "\n",
    "TASK2_IMAGES_DIR = BASE_DIR / \"Task2\" / \"images\"\n",
    "\n",
    "# Output paths\n",
    "TASK1_INPUT_VIDEO  = Path(\"task1_input.mp4\")\n",
    "TASK1_OUTPUT_VIDEO = Path(\"task1.mp4\")\n",
    "TASK2_OUTPUT_VIDEO = Path(\"task2.mp4\")\n",
    "TASK2_COUNTS_CSV   = Path(\"task2_count.csv\")\n",
    "\n",
    "FPS_TASK1 = 14\n",
    "FPS_TASK2 = 14\n",
    "\n",
    "# YOLO weights\n",
    "YOLO_WEIGHTS = \"yolov8x.pt\"  \n",
    "YOLO_IMGSZ = 1920       # using full-res\n",
    "YOLO_CONF = 0.25          # confidence threshold\n",
    "# Pick CUDA if available, otherwise CPU\n",
    "DEVICE = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55f22a",
   "metadata": {},
   "source": [
    "## 1. Data Preparation (Task 1 – images → video @ 14 FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da513638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Convert Task1 images to video (task1_input.mp4)\n",
    "# ============================================\n",
    "def images_to_video(image_dir: Path, output_path: Path, fps: int = 14):\n",
    "    \"\"\"\n",
    "    Convert all images in image_dir to a video at the given fps.\n",
    "    Assumes images are named so that lexicographic sort is correct frame order\n",
    "    (e.g., 000001.jpg, 000002.jpg, ...).\n",
    "    \"\"\"\n",
    "    image_files = sorted(\n",
    "        [p for p in image_dir.iterdir() if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    )\n",
    "    assert len(image_files) > 0, f\"No images found in {image_dir}\"\n",
    "\n",
    "    # Read first image to get frame size\n",
    "    first_frame = cv2.imread(str(image_files[0]))\n",
    "    assert first_frame is not None, f\"Could not read first image {image_files[0]}\"\n",
    "\n",
    "    height, width = first_frame.shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "\n",
    "    for img_path in image_files:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"Warning: could not read {img_path}, skipping.\")\n",
    "            continue\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Saved video: {output_path} ({len(image_files)} frames at {fps} FPS)\")\n",
    "\n",
    "# Run for Task1\n",
    "images_to_video(TASK1_IMAGES_DIR, TASK1_INPUT_VIDEO, fps=FPS_TASK1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522e94d",
   "metadata": {},
   "source": [
    "## 2. YOLOv8 + DeepSORT Tracking (Task 2 – Task1 video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1387df",
   "metadata": {},
   "source": [
    "### 2.1 Initialize YOLO and DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c7b56",
   "metadata": {},
   "source": [
    "We use YOLOv8 for pedestrian detection and DeepSORT to maintain consistent IDs across frames. Each frame is processed, detections are filtered to \"person\", and DeepSORT assigns a track ID. The output is both an annotated video and a MOT-format tracking file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.1 Initialize YOLOv8 and DeepSORT\n",
    "# ============================================\n",
    "def init_yolo(weights_path: str = YOLO_WEIGHTS, device: str = DEVICE):\n",
    "    \"\"\"\n",
    "    Initialize YOLOv8 model on CPU or CUDA if available.\n",
    "    \"\"\"\n",
    "    model = YOLO(weights_path)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_deepsort():\n",
    "    \"\"\"\n",
    "    Initialize DeepSort tracker from deep_sort_realtime.\n",
    "    \"\"\"\n",
    "    tracker = DeepSort(\n",
    "        max_age=45,\n",
    "        n_init=3,\n",
    "        nn_budget=100,\n",
    "        max_iou_distance=0.7,\n",
    "        max_cosine_distance=0.2,\n",
    "        embedder=\"mobilenet\",\n",
    "        half=False,\n",
    "        bgr=True,\n",
    "        embedder_gpu=True,\n",
    "    )\n",
    "    return tracker\n",
    "\n",
    "\n",
    "yolo_model = init_yolo()\n",
    "deepsort_tracker = init_deepsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b86739",
   "metadata": {},
   "source": [
    "### Optional sharpening and brightening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76421ec1",
   "metadata": {},
   "source": [
    "Gamma correction and sharpening help improve contrast and edge clarity in frames. This makes small or low-contrast pedestrians slightly easier for the detector to recognize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global hyperparameters for preprocessing\n",
    "GAMMA = 1.3         # brightness gamma\n",
    "SHARP_SIGMA = 0.9   # Gaussian blur sigma for unsharp mask\n",
    "SHARP_AMOUNT = 1.8   # sharpening strength\n",
    "\n",
    "\n",
    "def preprocess_frame(frame_bgr):\n",
    "    \"\"\"\n",
    "    Preprocessing before detection/tracking:\n",
    "      - apply gamma on V channel (brightness)\n",
    "      - apply mild unsharp masking for sharpening\n",
    "    Uses global GAMMA, SHARP_SIGMA, SHARP_AMOUNT.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Gamma on brightness (HSV V channel) ---\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    v_float = v.astype(np.float32) / 255.0\n",
    "    # gamma < 1 => brighter, gamma > 1 => darker\n",
    "    v_gamma = np.power(v_float, GAMMA)\n",
    "    v_new = np.clip(v_gamma * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    hsv = cv2.merge([h, s, v_new])\n",
    "    img_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # --- 2) Sharpening (unsharp mask) ---\n",
    "    if SHARP_AMOUNT > 0:\n",
    "        blurred = cv2.GaussianBlur(img_bgr, (0, 0), SHARP_SIGMA)\n",
    "        sharp = cv2.addWeighted(\n",
    "            img_bgr,\n",
    "            1.0 + SHARP_AMOUNT,\n",
    "            blurred,\n",
    "            -SHARP_AMOUNT,\n",
    "            0,\n",
    "        )\n",
    "        return sharp\n",
    "    else:\n",
    "        # no sharpening\n",
    "        return img_bgr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a58de",
   "metadata": {},
   "source": [
    "### 2.2 Helper: Run tracker on a video & save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ecc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.2 Run YOLOv8x + DeepSORT on Task1 video\n",
    "#     with darken+sharpen preprocessing\n",
    "# ============================================\n",
    "def run_tracking(\n",
    "    input_video_path: Path,\n",
    "    output_video_path: Path,\n",
    "    tracker_txt_out: Path,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps: int,\n",
    "    conf: float = YOLO_CONF,\n",
    "    imgsz: int = YOLO_IMGSZ,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 + DeepSORT tracking on a video.\n",
    "\n",
    "    Outputs:\n",
    "      - Annotated video with tracking boxes & IDs\n",
    "      - Text file with tracking results:\n",
    "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "    Uses preprocess_frame() for detection (darken + sharpen).\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(input_video_path))\n",
    "    assert cap.isOpened(), f\"Cannot open {input_video_path}\"\n",
    "\n",
    "    # Get frame size from first frame\n",
    "    ret, first_frame = cap.read()\n",
    "    assert ret, \"Could not read first frame\"\n",
    "    height, width = first_frame.shape[:2]\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    all_tracks = []  # (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height)\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_idx += 1\n",
    "\n",
    "        # --- Preprocess for detector (darken + sharpen) ---\n",
    "        frame_proc = preprocess_frame(frame)\n",
    "\n",
    "        height, width = frame_proc.shape[:2]\n",
    "\n",
    "        # YOLO inference on preprocessed frame\n",
    "        results = yolo_model(frame_proc, imgsz=imgsz, conf=conf, verbose=False)[0]\n",
    "        boxes = results.boxes\n",
    "\n",
    "        detections = []\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy  = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss  = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for bbox, score, cls in zip(xyxy, confs, clss):\n",
    "                # COCO class 0 = 'person'\n",
    "                if int(cls) != 0:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = bbox\n",
    "\n",
    "                # Clamp to image\n",
    "                x1 = max(0.0, min(x1, width - 1.0))\n",
    "                x2 = max(0.0, min(x2, width - 1.0))\n",
    "                y1 = max(0.0, min(y1, height - 1.0))\n",
    "                y2 = max(0.0, min(y2, height - 1.0))\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "\n",
    "                # DeepSORT expects [x, y, w, h]\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                detections.append(([x1, y1, w, h], float(score), \"person\"))\n",
    "\n",
    "        # Update DeepSORT using preprocessed frame\n",
    "        tracks = deepsort_tracker.update_tracks(detections, frame=frame_proc)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            # allow track to be drawn for up to 2 frames without an update\n",
    "            if track.time_since_update > 0:\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "\n",
    "            # Get left, top, right, bottom from tracker\n",
    "            l, t, r, b = track.to_ltrb()\n",
    "            l = max(0, min(int(l), width - 1))\n",
    "            r = max(0, min(int(r), width - 1))\n",
    "            t = max(0, min(int(t), height - 1))\n",
    "            b = max(0, min(int(b), height - 1))\n",
    "\n",
    "            bb_left   = float(l)\n",
    "            bb_top    = float(t)\n",
    "            bb_width  = float(r - l)\n",
    "            bb_height = float(b - t)\n",
    "\n",
    "            all_tracks.append(\n",
    "                (frame_idx, int(track_id), bb_left, bb_top, bb_width, bb_height)\n",
    "            )\n",
    "\n",
    "            # Draw on preprocessed frame\n",
    "            cv2.rectangle(frame_proc, (l, t), (r, b), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame_proc,\n",
    "                f\"ID {track_id}\",\n",
    "                (l, t - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        out.write(frame_proc)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Save tracking results to txt\n",
    "    tracker_txt_out = Path(tracker_txt_out)\n",
    "    with tracker_txt_out.open(\"w\") as f:\n",
    "        for (frame_idx, track_id, bb_left, bb_top, bb_width, bb_height) in all_tracks:\n",
    "            f.write(\n",
    "                f\"{frame_idx},{track_id},{bb_left:.2f},{bb_top:.2f},\"\n",
    "                f\"{bb_width:.2f},{bb_height:.2f}\\n\"\n",
    "            )\n",
    "\n",
    "    print(f\"Tracking done. Saved video to {output_video_path}\")\n",
    "    print(f\"Tracking results saved to {tracker_txt_out}\")\n",
    "\n",
    "\n",
    "# Run tracking for Task1\n",
    "TASK1_TRACKS_TXT = Path(\"task1_tracks.txt\")\n",
    "run_tracking(\n",
    "    TASK1_INPUT_VIDEO,\n",
    "    TASK1_OUTPUT_VIDEO,\n",
    "    TASK1_TRACKS_TXT,\n",
    "    yolo_model,\n",
    "    deepsort_tracker,\n",
    "    fps=FPS_TASK1,\n",
    "    conf=YOLO_CONF\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46ace0",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation: MOTA (Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be77b2",
   "metadata": {},
   "source": [
    "### 3.1 Load ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae07cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1 Load ground truth annotations (Task1/gt/gt.txt)\n",
    "# Using only the first 6 columns:\n",
    "# <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, ...\n",
    "# ============================================\n",
    "def load_gt(gt_path: Path):\n",
    "    \"\"\"\n",
    "    Load ground truth from MOT-style gt.txt.\n",
    "\n",
    "    Assumes columns:\n",
    "      1: frame\n",
    "      2: id\n",
    "      3: bb_left\n",
    "      4: bb_top\n",
    "      5: bb_width\n",
    "      6: bb_height\n",
    "      [7: conf (optional)]\n",
    "      [8: class (optional, 1 = pedestrian)]\n",
    "      [9+: other fields, ignored]\n",
    "\n",
    "    We:\n",
    "      - skip lines with conf <= 0 (unlabeled / ignored)\n",
    "      - if class column exists, keep only class == 1 (pedestrians)\n",
    "    \"\"\"\n",
    "    gt_by_frame = defaultdict(list)\n",
    "\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            cols = line.split(\",\")\n",
    "            if len(cols) < 6:\n",
    "                continue\n",
    "\n",
    "            frame = int(cols[0])\n",
    "            obj_id = int(cols[1])\n",
    "            bb_left   = float(cols[2])\n",
    "            bb_top    = float(cols[3])\n",
    "            bb_width  = float(cols[4])\n",
    "            bb_height = float(cols[5])\n",
    "\n",
    "            # Optional 7th column: conf\n",
    "            if len(cols) >= 7:\n",
    "                conf = float(cols[6])\n",
    "                # MOT convention: conf <= 0 => ignore\n",
    "                if conf <= 0:\n",
    "                    continue\n",
    "\n",
    "            # Optional 8th column: class (1 = pedestrian)\n",
    "            if len(cols) >= 8:\n",
    "                cls = int(cols[7])\n",
    "                if cls != 1:\n",
    "                    # keep only pedestrians\n",
    "                    continue\n",
    "\n",
    "            gt_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": obj_id,\n",
    "                    \"bbox\": [bb_left, bb_top, bb_width, bb_height],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return gt_by_frame\n",
    "\n",
    "gt_by_frame = load_gt(TASK1_GT_PATH)\n",
    "print(\"Loaded GT frames:\", len(gt_by_frame))\n",
    "print(\"Total GT boxes:\", sum(len(v) for v in gt_by_frame.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5095a93",
   "metadata": {},
   "source": [
    "### 3.2 Load predictions (tracker output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f878415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.2 Load tracking results from our tracker output txt\n",
    "# Format: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "# ============================================\n",
    "def load_predictions(pred_path: Path):\n",
    "    pred_by_frame = defaultdict(list)\n",
    "    with pred_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\",\")\n",
    "            frame = int(parts[0])\n",
    "            track_id = int(parts[1])\n",
    "            x = float(parts[2])\n",
    "            y = float(parts[3])\n",
    "            w = float(parts[4])\n",
    "            h = float(parts[5])\n",
    "\n",
    "            pred_by_frame[frame].append(\n",
    "                {\n",
    "                    \"id\": track_id,\n",
    "                    \"bbox\": np.array([x, y, w, h], dtype=float),\n",
    "                }\n",
    "            )\n",
    "    return pred_by_frame\n",
    "\n",
    "pred_by_frame = load_predictions(TASK1_TRACKS_TXT)\n",
    "print(f\"Loaded predictions for {len(pred_by_frame)} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efd6ff",
   "metadata": {},
   "source": [
    "### 3.3 IoU, Hungarian matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af910c6",
   "metadata": {},
   "source": [
    "We compute IoU between GT and predicted boxes and use the Hungarian algorithm to match pairs with IoU ≥ 0.5. Unmatched predictions = FP; unmatched GT = FN; switching matched IDs across frames = IDSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.3 IoU & matching utilities\n",
    "# ============================================\n",
    "def xywh_to_xyxy(box_xywh):\n",
    "    \"\"\"Convert [x, y, w, h] -> [x1, y1, x2, y2].\"\"\"\n",
    "    x, y, w, h = box_xywh\n",
    "    return np.array([x, y, x + w, y + h], dtype=float)\n",
    "\n",
    "\n",
    "def compute_iou_matrix(gt_boxes_xywh, pred_boxes_xywh):\n",
    "    \"\"\"\n",
    "    Compute IoU matrix between:\n",
    "      - gt_boxes_xywh: list of [x, y, w, h]\n",
    "      - pred_boxes_xywh: list of [x, y, w, h]\n",
    "    Returns: (N_gt, N_pred) IoU matrix.\n",
    "    \"\"\"\n",
    "    N = len(gt_boxes_xywh)\n",
    "    M = len(pred_boxes_xywh)\n",
    "\n",
    "    if N == 0 or M == 0:\n",
    "        return np.zeros((N, M), dtype=float)\n",
    "\n",
    "    gt = np.array([xywh_to_xyxy(b) for b in gt_boxes_xywh], dtype=float)  # (N,4)\n",
    "    pr = np.array([xywh_to_xyxy(b) for b in pred_boxes_xywh], dtype=float)  # (M,4)\n",
    "\n",
    "    gt_x1 = gt[:, 0][:, None]\n",
    "    gt_y1 = gt[:, 1][:, None]\n",
    "    gt_x2 = gt[:, 2][:, None]\n",
    "    gt_y2 = gt[:, 3][:, None]\n",
    "\n",
    "    pr_x1 = pr[:, 0][None, :]\n",
    "    pr_y1 = pr[:, 1][None, :]\n",
    "    pr_x2 = pr[:, 2][None, :]\n",
    "    pr_y2 = pr[:, 3][None, :]\n",
    "\n",
    "    inter_x1 = np.maximum(gt_x1, pr_x1)\n",
    "    inter_y1 = np.maximum(gt_y1, pr_y1)\n",
    "    inter_x2 = np.minimum(gt_x2, pr_x2)\n",
    "    inter_y2 = np.minimum(gt_y2, pr_y2)\n",
    "\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, a_min=0, a_max=None)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, a_min=0, a_max=None)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)   # (N,1)\n",
    "    pr_area = (pr_x2 - pr_x1) * (pr_y2 - pr_y1)   # (1,M)\n",
    "    union_area = gt_area + pr_area - inter_area\n",
    "\n",
    "    iou = np.zeros_like(inter_area)\n",
    "    mask = union_area > 0\n",
    "    iou[mask] = inter_area[mask] / union_area[mask]\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecceae6",
   "metadata": {},
   "source": [
    "### 3.4 Compute MOTA, FP, FN, IDSW, GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.4 Compute MOTA, FP, FN, IDSW, GT\n",
    "# ============================================\n",
    "def compute_mota(gt_by_frame, pred_by_frame, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute MOTA, and totals of FP, FN, IDSW, and GT.\n",
    "    Following the definition given in the assignment.\n",
    "    \"\"\"\n",
    "    frames = sorted(gt_by_frame.keys())       # frames with GT\n",
    "    all_frames = frames                       # just use GT frames (fixed for issue)\n",
    "\n",
    "\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "    total_IDSW = 0\n",
    "    total_GT = 0\n",
    "\n",
    "    # For ID switch tracking: gt_id -> last matched pred_id\n",
    "    prev_match_for_gt = {}\n",
    "\n",
    "    for t in all_frames:\n",
    "        gt_objs = gt_by_frame.get(t, [])\n",
    "        pr_objs = pred_by_frame.get(t, [])\n",
    "\n",
    "        gt_boxes = [g[\"bbox\"] for g in gt_objs]\n",
    "        gt_ids = [g[\"id\"] for g in gt_objs]\n",
    "\n",
    "        pr_boxes = [p[\"bbox\"] for p in pr_objs]\n",
    "        pr_ids = [p[\"id\"] for p in pr_objs]\n",
    "\n",
    "        N = len(gt_boxes)\n",
    "        M = len(pr_boxes)\n",
    "\n",
    "        total_GT += N\n",
    "\n",
    "        if N == 0 and M == 0:\n",
    "            # nothing here\n",
    "            continue\n",
    "\n",
    "        # IoU matrix\n",
    "        iou_mat = compute_iou_matrix(gt_boxes, pr_boxes)\n",
    "\n",
    "        if N > 0 and M > 0:\n",
    "            # Cost matrix for Hungarian: we want to maximize IoU,\n",
    "            # so we minimize (1 - IoU). Set cost very high if IoU < threshold.\n",
    "            cost = 1.0 - iou_mat\n",
    "            cost[iou_mat < iou_threshold] = 1e6\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_gt_idx = set()\n",
    "            matched_pr_idx = set()\n",
    "\n",
    "            # Evaluate matches above threshold\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if iou_mat[r, c] >= iou_threshold:\n",
    "                    matched_gt_idx.add(r)\n",
    "                    matched_pr_idx.add(c)\n",
    "\n",
    "                    gt_id = gt_ids[r]\n",
    "                    pr_id = pr_ids[c]\n",
    "\n",
    "                    # Identity switch\n",
    "                    if gt_id in prev_match_for_gt:\n",
    "                        if prev_match_for_gt[gt_id] != pr_id:\n",
    "                            total_IDSW += 1\n",
    "                    prev_match_for_gt[gt_id] = pr_id\n",
    "\n",
    "            # FN: GT with no match\n",
    "            FN_t = N - len(matched_gt_idx)\n",
    "\n",
    "            # FP: predictions with no match\n",
    "            FP_t = M - len(matched_pr_idx)\n",
    "\n",
    "        elif N == 0 and M > 0:\n",
    "            # All predictions are FP\n",
    "            FP_t = M\n",
    "            FN_t = 0\n",
    "\n",
    "        elif N > 0 and M == 0:\n",
    "            # All GT are FN\n",
    "            FN_t = N\n",
    "            FP_t = 0\n",
    "\n",
    "        total_FN += FN_t\n",
    "        total_FP += FP_t\n",
    "\n",
    "    if total_GT == 0:\n",
    "        mota = 0.0\n",
    "    else:\n",
    "        mota = 1.0 - (total_FN + total_FP + total_IDSW) / total_GT\n",
    "\n",
    "    return mota, total_FP, total_FN, total_IDSW, total_GT\n",
    "\n",
    "\n",
    "mota, total_FP, total_FN, total_IDSW, total_GT = compute_mota(\n",
    "    gt_by_frame, pred_by_frame, iou_threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"MOTA: {mota:.4f}\")\n",
    "print(f\"Total GT:   {total_GT}\")\n",
    "print(f\"Total FP:   {total_FP}\")\n",
    "print(f\"Total FN:   {total_FN}\")\n",
    "print(f\"Total IDSW: {total_IDSW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84cb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug: how many GT vs predictions per frame?\n",
    "all_gt_frames = sorted(gt_by_frame.keys())\n",
    "all_pred_frames = sorted(pred_by_frame.keys())\n",
    "\n",
    "total_GT = sum(len(gt_by_frame[f]) for f in all_gt_frames)\n",
    "total_pred = sum(len(pred_by_frame.get(f, [])) for f in all_gt_frames)\n",
    "\n",
    "print(\"Frames with GT:\", len(all_gt_frames))\n",
    "print(\"Total GT boxes:\", total_GT, \"-> avg per frame:\", total_GT / len(all_gt_frames))\n",
    "print(\"Total pred boxes on those frames:\", total_pred, \"-> avg per frame:\", total_pred / len(all_gt_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88a53b",
   "metadata": {},
   "source": [
    "## MOTA Score Explanation\n",
    "\n",
    "Our tracker achieved **MOTA = 0.47**. This is mainly due to:\n",
    "\n",
    "- **High FN (missed people):** Many pedestrians are far away, small, or occluded, so YOLOv8 fails to detect them.\n",
    "- **High FP (extra detections):** Background textures and shadows sometimes trigger false positives.\n",
    "- **Identity switches:** Pedestrians overlap and look very similar, causing DeepSORT to change IDs when tracks are lost or occluded.\n",
    "- **Low FPS (14):** Larger motion between frames makes tracking harder and increases ID switches.\n",
    "\n",
    "All these errors contribute directly to lowering the MOTA score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ecece",
   "metadata": {},
   "source": [
    "### DEBUG sweep sharpening brightness and confidence values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# Hyperparameter sweep: conf × gamma × sigma × amount\n",
    "# ============================================\n",
    "\n",
    "# Confidence values to try\n",
    "CONF_SWEEP = [0.29, 0.28, 0.27, 0.26, 0.25]\n",
    "\n",
    "# Preprocessing hyperparam grids \n",
    "GAMMA_SWEEP = [1.3]   \n",
    "SIGMA_SWEEP = [0.9]        \n",
    "AMOUNT_SWEEP = [1.8, 1.9]      \n",
    "\n",
    "sweep_results = []\n",
    "\n",
    "# Load GT once\n",
    "gt_by_frame = load_gt(TASK1_GT_PATH)\n",
    "\n",
    "for conf in CONF_SWEEP:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"### Sweeping hyperparams for conf = {conf:.2f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # For this conf, reuse same output filenames\n",
    "    tag = int(conf * 100)\n",
    "    task1_video_out  = Path(f\"task1_conf{tag:03d}.mp4\")\n",
    "    task1_tracks_out = Path(f\"task1_tracks_conf{tag:03d}.txt\")\n",
    "\n",
    "    for gamma, sigma, amount in itertools.product(GAMMA_SWEEP, SIGMA_SWEEP, AMOUNT_SWEEP):\n",
    "        # Set global preprocess hyperparameters\n",
    "        GAMMA = gamma\n",
    "        SHARP_SIGMA = sigma\n",
    "        SHARP_AMOUNT = amount\n",
    "\n",
    "        print(\n",
    "            f\"\\n--- conf={conf:.2f}, gamma={gamma:.2f}, \"\n",
    "            f\"sigma={sigma:.2f}, amount={amount:.2f} ---\"\n",
    "        )\n",
    "\n",
    "        # Fresh model + tracker for each combo\n",
    "        yolo_model_t1 = init_yolo()\n",
    "        deepsort_t1   = init_deepsort()\n",
    "\n",
    "        # Run tracking with these hyperparams\n",
    "        run_tracking(\n",
    "            TASK1_INPUT_VIDEO,\n",
    "            task1_video_out,\n",
    "            task1_tracks_out,\n",
    "            yolo_model_t1,\n",
    "            deepsort_t1,\n",
    "            fps=FPS_TASK1,\n",
    "            conf=conf,\n",
    "            imgsz=YOLO_IMGSZ,\n",
    "        )\n",
    "\n",
    "        # Compute MOTA for this combo\n",
    "        pred_by_frame = load_predictions(task1_tracks_out)\n",
    "\n",
    "        mota, total_FP, total_FN, total_IDSW, total_GT = compute_mota(\n",
    "            gt_by_frame, pred_by_frame, iou_threshold=0.5\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"MOTA={mota:.4f} | FP={total_FP} | FN={total_FN} | \"\n",
    "            f\"IDSW={total_IDSW} | GT={total_GT}\"\n",
    "        )\n",
    "\n",
    "        sweep_results.append(\n",
    "            {\n",
    "                \"conf\": conf,\n",
    "                \"gamma\": gamma,\n",
    "                \"sigma\": sigma,\n",
    "                \"amount\": amount,\n",
    "                \"MOTA\": mota,\n",
    "                \"FP\": total_FP,\n",
    "                \"FN\": total_FN,\n",
    "                \"IDSW\": total_IDSW,\n",
    "                \"GT\": total_GT,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Turn results into a DataFrame \n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "sweep_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4ecb8",
   "metadata": {},
   "source": [
    "## 4. Prediction & Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62542e3",
   "metadata": {},
   "source": [
    "### 4.1 Convert Task2 images to a video and track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee448eff",
   "metadata": {},
   "source": [
    "## Task 2 Counting Method\n",
    "\n",
    "For each frame, we use Faster R-CNN to detect pedestrians and count all detection boxes above a chosen confidence threshold. These per-frame counts are saved in a CSV file for Kaggle evaluation.\n",
    "\n",
    "Although we implemented DeepSORT for Task 2 as well, we found during testing that **using only the detector (no tracker) and tuning the confidence threshold produced a better Kaggle score**. The tracker's identity persistence is not needed for counting and sometimes increases false positives.\n",
    "\n",
    "However, **the assignment requires an annotated tracking video**, so our submitted `task2.mp4` uses the tracker, while the Kaggle CSV uses the detector-only counts.\n",
    "\n",
    "## Reason for Using Faster R-CNN\n",
    "\n",
    "We chose Faster R-CNN because it performs more reliably on medium-sized pedestrians and produces more stable scores across frames compared to YOLOv8. This stability makes confidence-based counting more consistent, especially when not using a tracker for the Kaggle submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.1 Task2: Faster R-CNN + DeepSORT on Task2 images\n",
    "#     - Preprocess frames\n",
    "#     - Build input video (task2_input.mp4)\n",
    "#     - Build tracked video (task2.mp4)\n",
    "#     - Save tracking results (task2_tracks.txt)\n",
    "#     - Return per-frame counts for CSV\n",
    "# ============================================\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "\n",
    "# cv2, np, DeepSort imported earlier in the notebook.\n",
    "\n",
    "# Task2-specific settings \n",
    "DEVICE_T2       = \"cuda\"   \n",
    "FPS_TASK2       = 14.0     \n",
    "SCORE_THRESH_T2 = 0.475    # detector score threshold\n",
    "\n",
    "# Paths for Task2\n",
    "TASK2_INPUT_VIDEO  = Path(\"task2_input.mp4\")\n",
    "TASK2_TRACKS_TXT   = Path(\"task2_tracks.txt\")\n",
    "\n",
    "\n",
    "def load_frcnn_detector(device: str = DEVICE_T2):\n",
    "    \"\"\"\n",
    "    Load a Faster R-CNN ResNet50 FPN v2 detector with COCO weights.\n",
    "    \"\"\"\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA requested but not available, falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def init_deepsort_task2():\n",
    "    \"\"\"\n",
    "    Initialize DeepSort tracker for Task2.\n",
    "    \"\"\"\n",
    "    tracker = DeepSort(\n",
    "        max_age=30,\n",
    "        n_init=3,\n",
    "        max_iou_distance=0.7,\n",
    "        nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.2,\n",
    "        embedder=\"mobilenet\",\n",
    "        half=False,\n",
    "        bgr=True,\n",
    "        embedder_gpu=True,\n",
    "    )\n",
    "    return tracker\n",
    "\n",
    "\n",
    "def preprocess_frame(frame_bgr):\n",
    "    \"\"\"\n",
    "    Darken bright regions and sharpen the image to help detector.\n",
    "    \"\"\"\n",
    "    # 1) Darken highlights using gamma on V channel in HSV\n",
    "    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    v_float = v.astype(np.float32) / 255.0\n",
    "    gamma = 1.4\n",
    "    v_gamma = np.power(v_float, gamma)\n",
    "    v_new = np.clip(v_gamma * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    hsv = cv2.merge([h, s, v_new])\n",
    "    img_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # 2) Stronger sharpening (unsharp mask)\n",
    "    sigma = 1.2\n",
    "    amount = 1.8\n",
    "    blurred = cv2.GaussianBlur(img_bgr, (0, 0), sigma)\n",
    "    sharp = cv2.addWeighted(img_bgr, 1.0 + amount, blurred, -amount, 0)\n",
    "\n",
    "    return sharp\n",
    "\n",
    "\n",
    "def process_task2_frames(\n",
    "    frames_dir: Path,\n",
    "    input_video_path: Path,\n",
    "    tracked_video_path: Path,\n",
    "    tracks_txt_path: Path,\n",
    "    fps: float = FPS_TASK2,\n",
    "    device: str = DEVICE_T2,\n",
    "    score_thresh: float = SCORE_THRESH_T2,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Reads Task2 frames from frames_dir\n",
    "    - Builds a raw input video (input_video_path)\n",
    "    - Runs Faster R-CNN + DeepSORT on preprocessed frames\n",
    "    - Builds a tracked video (tracked_video_path)\n",
    "    - Saves MOT-style tracks to tracks_txt_path\n",
    "    - Returns: dict[frame_idx] -> person_count (from detector boxes)\n",
    "    \"\"\"\n",
    "    # Load detector + tracker\n",
    "    model, device = load_frcnn_detector(device)\n",
    "    tracker = init_deepsort_task2()\n",
    "\n",
    "    image_paths = sorted(frames_dir.glob(\"*.jpg\"))\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(f\"No .jpg files found in {frames_dir}\")\n",
    "\n",
    "    # Read first frame to get size\n",
    "    first_frame = cv2.imread(str(image_paths[0]))\n",
    "    if first_frame is None:\n",
    "        raise RuntimeError(f\"Could not read first frame: {image_paths[0]}\")\n",
    "\n",
    "    height, width = first_frame.shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "    # Video writers\n",
    "    input_writer = cv2.VideoWriter(str(input_video_path), fourcc, fps, (width, height))\n",
    "    tracked_writer = cv2.VideoWriter(str(tracked_video_path), fourcc, fps, (width, height))\n",
    "\n",
    "    # Outputs\n",
    "    track_lines = []      # for MOT-style txt\n",
    "    frame_counts = {}     # frame_idx -> person_count\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_path in image_paths:\n",
    "            frame_idx += 1\n",
    "\n",
    "            frame_bgr = cv2.imread(str(img_path))\n",
    "            if frame_bgr is None:\n",
    "                print(f\"WARNING: could not read {img_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            original_bgr = frame_bgr.copy()\n",
    "\n",
    "            # Preprocess for detector (Optional)\n",
    "            # frame_bgr = preprocess_frame(frame_bgr)\n",
    "\n",
    "            # Write original raw frame to input video\n",
    "            input_writer.write(original_bgr)\n",
    "\n",
    "            # Prepare image for detector (RGB) using preprocessed frame\n",
    "            rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(rgb)\n",
    "            tensor = F.to_tensor(pil_img).to(device)\n",
    "\n",
    "            outputs = model([tensor])[0]\n",
    "            boxes = outputs[\"boxes\"].cpu()\n",
    "            labels = outputs[\"labels\"].cpu()\n",
    "            scores = outputs[\"scores\"].cpu()\n",
    "\n",
    "            detections = []\n",
    "            person_count = 0\n",
    "\n",
    "            # Build detections for DeepSort\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if label.item() != 1:  # COCO: 1 = person\n",
    "                    continue\n",
    "                if score.item() < score_thresh:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "                # Clamp to frame\n",
    "                x1 = max(0.0, min(x1, width - 1.0))\n",
    "                x2 = max(0.0, min(x2, width - 1.0))\n",
    "                y1 = max(0.0, min(y1, height - 1.0))\n",
    "                y2 = max(0.0, min(y2, height - 1.0))\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                detections.append(([x1, y1, w, h], float(score.item()), \"person\"))\n",
    "                person_count += 1\n",
    "\n",
    "            # Update tracker\n",
    "            tracks = tracker.update_tracks(detections, frame=frame_bgr)\n",
    "\n",
    "            # Draw tracks and build MOT lines\n",
    "            for trk in tracks:\n",
    "                if not trk.is_confirmed() or trk.time_since_update > 0:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, trk.to_ltrb())\n",
    "\n",
    "                # Clamp\n",
    "                x1 = max(0, min(x1, width - 1))\n",
    "                x2 = max(0, min(x2, width - 1))\n",
    "                y1 = max(0, min(y1, height - 1))\n",
    "                y2 = max(0, min(y2, height - 1))\n",
    "\n",
    "                bb_left = x1\n",
    "                bb_top = y1\n",
    "                bb_width = max(0, x2 - x1)\n",
    "                bb_height = max(0, y2 - y1)\n",
    "                track_id = trk.track_id\n",
    "\n",
    "                # Draw rectangle + ID\n",
    "                cv2.rectangle(\n",
    "                    frame_bgr,\n",
    "                    (bb_left, bb_top),\n",
    "                    (bb_left + bb_width, bb_top + bb_height),\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame_bgr,\n",
    "                    f\"ID {track_id}\",\n",
    "                    (bb_left, max(0, bb_top - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0, 255, 0),\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                # MOT-style line: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "                track_lines.append(\n",
    "                    f\"{frame_idx}, {track_id}, {bb_left}, {bb_top}, {bb_width}, {bb_height}\\n\"\n",
    "                )\n",
    "\n",
    "            # Write tracked frame (preprocessed + annotations)\n",
    "            tracked_writer.write(frame_bgr)\n",
    "\n",
    "            # Save count for this frame (detector-based, not unique IDs)\n",
    "            frame_counts[frame_idx] = person_count\n",
    "\n",
    "            print(\n",
    "                f\"Frame {frame_idx:4d}: {person_count:2d} people,\"\n",
    "                f\" {len(detections)} detections, {len(tracks)} tracks\"\n",
    "            )\n",
    "\n",
    "    # Release videos\n",
    "    input_writer.release()\n",
    "    tracked_writer.release()\n",
    "\n",
    "    # Write tracks txt\n",
    "    tracks_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with tracks_txt_path.open(\"w\") as f:\n",
    "        f.writelines(track_lines)\n",
    "\n",
    "    print(f\"\\nSaved input video to   {input_video_path}\")\n",
    "    print(f\"Saved tracked video to {tracked_video_path}\")\n",
    "    print(f\"Saved tracks to        {tracks_txt_path}\")\n",
    "\n",
    "    return frame_counts\n",
    "\n",
    "\n",
    "# ---- Run Task2 processing and get per-frame counts ----\n",
    "task2_frame_counts = process_task2_frames(\n",
    "    frames_dir=TASK2_IMAGES_DIR,\n",
    "    input_video_path=TASK2_INPUT_VIDEO,\n",
    "    tracked_video_path=TASK2_OUTPUT_VIDEO,\n",
    "    tracks_txt_path=TASK2_TRACKS_TXT,\n",
    "    fps=FPS_TASK2,\n",
    "    device=DEVICE_T2,\n",
    "    score_thresh=SCORE_THRESH_T2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ede7cc",
   "metadata": {},
   "source": [
    "### 4.2 Save frame counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.2 Save Task2 frame counts to CSV for Kaggle\n",
    "#     Format:\n",
    "#       Number,Count\n",
    "#       1,12\n",
    "#       2,15\n",
    "#       ...\n",
    "# ============================================\n",
    "def save_counts_to_csv(frame_counts: dict, csv_path: Path):\n",
    "    \"\"\"\n",
    "    frame_counts: dict[frame_idx] -> count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for frame_idx in sorted(frame_counts.keys()):\n",
    "        data.append({\"Number\": frame_idx, \"Count\": frame_counts[frame_idx]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved counts to {csv_path}\")\n",
    "\n",
    "\n",
    "save_counts_to_csv(task2_frame_counts, TASK2_COUNTS_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc8ba4",
   "metadata": {},
   "source": [
    "## Kaggle Score Explanation\n",
    "\n",
    "Our Kaggle RMSE score was **2.38**. Errors mainly come from:\n",
    "\n",
    "- **Overcounting:** Duplicate detections on the same person in crowded frames.\n",
    "- **Undercounting:** Small or distant pedestrians are often missed.\n",
    "- **Threshold sensitivity:** The detector score threshold was not fully optimized.\n",
    "\n",
    "These factors cause the predicted counts to differ from the ground truth by ~2–3 people on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729b82c",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "We built videos, tracked pedestrians, evaluated tracking with MOTA, and generated per-frame counts for Task 2. The results depend heavily on detection quality, occlusions, and threshold choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
